{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Prompt Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 1. Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1699507327585
        }
      },
      "outputs": [],
      "source": [
        "from openai import AzureOpenAI\n",
        "import os\n",
        "\n",
        "base_url = \"https://aoai719xyz01.openai.azure.com/\"\n",
        "api_version = \"2023-07-01-preview\"\n",
        "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "deployment_name = \"gpt35turbo-16k\" # change to your deployment name\n",
        "\n",
        "client = AzureOpenAI(api_key=api_key, api_version=api_version, azure_endpoint=base_url)\n",
        "\n",
        "# call OpenAI API with model name and prompt\n",
        "def call_openai_api(prompt, deployment_name=deployment_name, max_token=400, stop=None, n=1, temperature=0):\n",
        "    response = client.chat.completions.create(\n",
        "    model=deployment_name,\n",
        "    messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant. \"},\n",
        "                {\"role\":\"user\",\"content\": prompt,}],\n",
        "        max_tokens=max_token,\n",
        "        stop=stop,\n",
        "        n=n,\n",
        "        temperature=temperature,)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 2. Chain of Thought\n",
        "\n",
        "Experiment results demonstrate Zero-shot-CoT using single prompt template, significantly outperform zero-shot LLM performance on diverse benchmark reasoning tasks. Without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002).\n",
        "\n",
        "Source: [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)\n",
        "\n",
        "##### Recent GPT-35-Turbo has improved logic reasoning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1699507408211
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.\n"
          ]
        }
      ],
      "source": [
        "# This prompt gets wrong answer\n",
        "\n",
        "PROMPT_ZERO_SHOT = \"\"\"Q: A juggler can juggle 16 balls. Half of the balls are golf balls,\n",
        "and half of the golf balls are blue. How many blue golf balls are\n",
        "there?\n",
        "A: The answer (arabic numerals) is\n",
        "\"\"\"\n",
        "response = call_openai_api(PROMPT_ZERO_SHOT)\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### However, it still makes mistakes with one-shot learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1699507540998
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 8 blue golf balls.\n"
          ]
        }
      ],
      "source": [
        "# Still wrong answer with few-shot learning\n",
        "\n",
        "PROMPT_FEW_SHOT = \"\"\"Q: Roger has 5 tennis balss. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does Roger have now?\n",
        "A: The answer is 11.\n",
        "\n",
        "Q: A juggler can juggle 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?\n",
        "A:\n",
        "\"\"\"\n",
        "response = call_openai_api(PROMPT_FEW_SHOT)\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1699507588118
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First, we need to find out how many golf balls there are in total. Since half of the balls are golf balls, we can divide the total number of balls by 2. So, 16 balls divided by 2 equals 8 golf balls.\n",
            "\n",
            "Next, we need to find out how many of these golf balls are blue. Since half of the golf balls are blue, we can again divide the number of golf balls by 2. So, 8 golf balls divided by 2 equals 4 blue golf balls.\n",
            "\n",
            "Therefore, there are 4 blue golf balls.\n"
          ]
        }
      ],
      "source": [
        "# With CoT, the answer is correct\n",
        "\n",
        "PROMPT_ZERO_SHOT_CoT = \"\"\"Q: A juggler can juggle 16 balls. Half of the balls are golf balls,\n",
        "and half of the golf balls are blue. How many blue golf balls are\n",
        "there?\n",
        "A: Letâ€™s think step by step.\n",
        "\"\"\"\n",
        "response = call_openai_api(PROMPT_ZERO_SHOT_CoT)\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1699507675009
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "If half of the balls are golf balls, then there are 16/2 = 8 golf balls.\n",
            "If half of the golf balls are blue, then there are 8/2 = 4 blue golf balls.\n",
            "Therefore, there are 4 blue golf balls.\n"
          ]
        }
      ],
      "source": [
        "PROMPT_FEW_SHOT_CoT = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis\n",
        "balls. Each can has 3 tennis balls. How many tennis balls does\n",
        "he have now?\n",
        "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6\n",
        "tennis balls. 5 + 6 = 11. The answer is 11.\n",
        "Q: A juggler can juggle 16 balls. Half of the balls are golf balls,\n",
        "and half of the golf balls are blue. How many blue golf balls are\n",
        "there?\n",
        "A:\n",
        "\"\"\"\n",
        "response = call_openai_api(PROMPT_FEW_SHOT_CoT)\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Commonsense Reasoning\n",
        "\n",
        "Paper: [Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/abs/2110.08387)\n",
        "\n",
        "Provide knowledge, turn knowledge question into reasoning. In general, more knowledge, better result.\n",
        "\n",
        "3 Contributing factors:\n",
        "\n",
        "(i) the quality of knowledge,\n",
        "\n",
        "(ii) the quantity of knowledge where the performance improves with more knowledge statements, and\n",
        "\n",
        "(iii) the strategy for integrating knowledge during inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1699508241666
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False. In golf, the objective is to have the lowest score possible. The player with the lowest score is the winner.\n"
          ]
        }
      ],
      "source": [
        "PROMPT = \"\"\"The player with the lowest score wins.\n",
        "Is this true or false: Part of golf is trying to get a higher point total than others.\n",
        "\"\"\"\n",
        "response = call_openai_api(PROMPT)\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1699508243826
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An easel typically has three legs.\n"
          ]
        }
      ],
      "source": [
        "PROMPT = \"\"\"A tripod is a kind of easel\n",
        "How many legs does an easel have?\n",
        "\"\"\"\n",
        "response = call_openai_api(PROMPT)\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
